{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Churn Rate\n\n## Project Overview\n\nThis project made to obtain grades for Reproducible Research class at the University of Warsaw.  \n\nOur group:\n1. Orkhan Amrullayev\n2. Srinesh Heshan\n3. Laura Florencia\n\nThe project's aim to predict behaviour of teleco customer. The dataset itself comes from kaggle (URL at the reference). Here we would like to analyze from the customer's data then see the churn implies to retention approach based on one base paper and we do the improvement. \n\nWe will predict how the customer will churn by analyze the details:  \n* Account information\n* Demographic information\n* Services information  \n\nBy those parameters, we will also know how to increase the customer satisfaction and somehow improve the previous research. We are now using Tensorflow library as the differenciate and some algorithms to get the different result.","metadata":{}},{"cell_type":"markdown","source":"## Weka Data Mining Tool\n\nHere they have used weka which is a data mining tool for small scale projects. \n\nWeka features include machine learning, data mining, preprocessing, classification, regression, clustering, association rules, attribute selection, experiments, workflow and visualization. Weka is written in Java, developed at the University of Waikato, New Zealand.","metadata":{}},{"cell_type":"markdown","source":"## Google Colab\n\nWhy do we using Colab?\n\nColaboratory, or “Colab” for short, is a product from Google Research. Colab allows anybody to write and execute arbitrary python code through the browser, and is especially well suited to machine learning, data analysis and education.\n\n\nNotebooks can stay connected for up to 24 hours, compared to the 12 hours in the free version of Colab notebooks. Get priority access to high-memory VMs. These VMs generally have double the memory of standard Colab VMs and twice as many CPUs. Users might even be automatically given a high-memory VM when Colab detects that the need. Another feature is absent in the free version. To offer faster GPUs, longer runtimes and more memory in Colab for a relatively low price, Google needs to maintain the flexibility to adjust usage limits and the availability of hardware on the fly. \n\nResources in Colab Pro are prioritised for subscribers who have recently used fewer resources, in order to prevent the monopolisation of limited resources by a small number of users. To get the most out of Colab Pro, consider closing your Colab tabs when you are done with your work, and avoid opting for GPUs or extra memory when it is not needed for your work. \nThis will make it less likely for a user to run into usage limits within Colab Pro.","metadata":{}},{"cell_type":"markdown","source":"# Dataset Detail  \n\nDataset contains 7043 rows and 34 columns. The rows comes from customers data and the column represent the features in dataset. However, we will just drop some columns, and will only use 19 independet variables and 1 dependent variables.\n\nTarget variable/ dependent variable: column \"churn value\" (we also have \"churn label\" in the column, but we will use the \"churn value\" as it is easy to use - integer value).  \n\nIndependent variables: 19 columns which represent the characteristics of customers.  ","metadata":{}},{"cell_type":"markdown","source":"# Load libraries\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\n\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.utils import resample\n\n%matplotlib inline\npd.options.display.max_columns = 500\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:43:13.260939Z","iopub.execute_input":"2022-05-25T21:43:13.261328Z","iopub.status.idle":"2022-05-25T21:43:13.273368Z","shell.execute_reply.started":"2022-05-25T21:43:13.261296Z","shell.execute_reply":"2022-05-25T21:43:13.272221Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"# Load data\n\nLoad the dataset that we are using as the source of the research.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('Churn_dataset.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:43:16.839738Z","iopub.execute_input":"2022-05-25T21:43:16.840150Z","iopub.status.idle":"2022-05-25T21:43:16.880228Z","shell.execute_reply.started":"2022-05-25T21:43:16.840114Z","shell.execute_reply":"2022-05-25T21:43:16.879209Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA)\n\nWe are showing the small detail of the dataset.","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:43:50.680553Z","iopub.execute_input":"2022-05-25T21:43:50.681105Z","iopub.status.idle":"2022-05-25T21:43:50.709706Z","shell.execute_reply.started":"2022-05-25T21:43:50.681058Z","shell.execute_reply":"2022-05-25T21:43:50.708793Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:43:51.554453Z","iopub.execute_input":"2022-05-25T21:43:51.554830Z","iopub.status.idle":"2022-05-25T21:43:51.593488Z","shell.execute_reply.started":"2022-05-25T21:43:51.554797Z","shell.execute_reply":"2022-05-25T21:43:51.592648Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"Apparently we have no null values, however TotalCharges has incorrect format (object), fix this converting it to float (float64) and filling missing values those will be generated during conversion with 0.","metadata":{}},{"cell_type":"code","source":"df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\ndf['TotalCharges'] = df['TotalCharges'].fillna(value=0)\n\ndf['tenure'] = df['tenure'].astype('float64')","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:43:54.244181Z","iopub.execute_input":"2022-05-25T21:43:54.244711Z","iopub.status.idle":"2022-05-25T21:43:54.257617Z","shell.execute_reply.started":"2022-05-25T21:43:54.244676Z","shell.execute_reply":"2022-05-25T21:43:54.256466Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"Drop customer ID as it's not relevant field for analysis.","metadata":{}},{"cell_type":"code","source":"df.drop('customerID', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:43:55.111467Z","iopub.execute_input":"2022-05-25T21:43:55.112129Z","iopub.status.idle":"2022-05-25T21:43:55.119415Z","shell.execute_reply.started":"2022-05-25T21:43:55.112091Z","shell.execute_reply":"2022-05-25T21:43:55.118464Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"Splitting categorical and numerical columns","metadata":{}},{"cell_type":"code","source":"col_cat = df.select_dtypes(include='object').drop('Churn', axis=1).columns.tolist()\ncol_num = df.select_dtypes(exclude='object').columns.tolist()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:43:57.049510Z","iopub.execute_input":"2022-05-25T21:43:57.050103Z","iopub.status.idle":"2022-05-25T21:43:57.061816Z","shell.execute_reply.started":"2022-05-25T21:43:57.050050Z","shell.execute_reply":"2022-05-25T21:43:57.060654Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"# Data Analysis in Categorical Fields","metadata":{}},{"cell_type":"markdown","source":"For our categorical fields, check how many unique values has each column so we will decide if feature engineering (and merging values in case there is too many of them) is needed.\nYou will see we have 2-4 unique values that is ideal.","metadata":{}},{"cell_type":"code","source":"for c in col_cat:\n    print('Column {} unique values: {}'.format(c, len(df[c].unique())))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:43:59.415104Z","iopub.execute_input":"2022-05-25T21:43:59.415519Z","iopub.status.idle":"2022-05-25T21:43:59.435634Z","shell.execute_reply.started":"2022-05-25T21:43:59.415481Z","shell.execute_reply":"2022-05-25T21:43:59.434652Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"Let's look at the distribution of Churn by all categorical variables. We can see that gender is not correlated with Churn at all, but Contract is highly correlated with churn and customers with contract of month-to-month are more likely to churn than the customers with 1-year and 2-years contracts. It may lead company to promote 1 & 2 years contract.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\nfor i,c in enumerate(col_cat):\n    plt.subplot(5,4,i+1)\n    sns.countplot(df[c], hue=df['Churn'])\n    plt.title(c)\n    plt.xlabel('')","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:44:04.256019Z","iopub.execute_input":"2022-05-25T21:44:04.256626Z","iopub.status.idle":"2022-05-25T21:44:06.622879Z","shell.execute_reply.started":"2022-05-25T21:44:04.256588Z","shell.execute_reply":"2022-05-25T21:44:06.621994Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"Here we have distribution of our numerical features\nIt seems tenure is correlated with Churn.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,5))\nfor i,c in enumerate(['tenure', 'MonthlyCharges', 'TotalCharges']):\n    plt.subplot(1,3,i+1)\n    sns.distplot(df[df['Churn'] == 'No'][c], kde=True, color='blue', hist=False, kde_kws=dict(linewidth=2), label='No')\n    sns.distplot(df[df['Churn'] == 'Yes'][c], kde=True, color='Orange', hist=False, kde_kws=dict(linewidth=2), label='Yes')\n    plt.title(c)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:44:09.090191Z","iopub.execute_input":"2022-05-25T21:44:09.090573Z","iopub.status.idle":"2022-05-25T21:44:09.627417Z","shell.execute_reply.started":"2022-05-25T21:44:09.090530Z","shell.execute_reply":"2022-05-25T21:44:09.626282Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"### Violin plot\n\nA violin plot is a hybrid of a box plot and a kernel density plot, *which shows peaks in the data*. It is used to visualize the distribution of numerical data of our variable which we prepared before. Unlike a box plot that can only show summary statistics, violin plots depict summary statistics and the density of each variable.  \n\nThis violin plot shows the relationship of feed type to chick weight. The box plot elements show the median weight for horsebean-fed chicks is lower than for other feed types. The shape of the distribution (extremely skinny on each end and wide in the middle) indicates the weights of sunflower-fed chicks are highly concentrated around the median.  ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,5))\nfor i,c in enumerate(col_num):\n    plt.subplot(1,4,i+1)\n    sns.violinplot(x=df['Churn'], y=df[c])\n    plt.title(c)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:44:14.316086Z","iopub.execute_input":"2022-05-25T21:44:14.316472Z","iopub.status.idle":"2022-05-25T21:44:14.918873Z","shell.execute_reply.started":"2022-05-25T21:44:14.316439Z","shell.execute_reply":"2022-05-25T21:44:14.917683Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"# Data preprocessing\n\nAfter EDA, let's prepare our date for the machine learning algorithms. One hot encoding of our categorical features. ","metadata":{}},{"cell_type":"markdown","source":"One hot encoding of our categorical features.","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:44:16.875820Z","iopub.execute_input":"2022-05-25T21:44:16.876229Z","iopub.status.idle":"2022-05-25T21:44:16.902972Z","shell.execute_reply.started":"2022-05-25T21:44:16.876193Z","shell.execute_reply":"2022-05-25T21:44:16.901850Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"dfT = pd.get_dummies(df, columns=col_cat)\ndfT.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:44:18.927709Z","iopub.execute_input":"2022-05-25T21:44:18.928434Z","iopub.status.idle":"2022-05-25T21:44:18.989784Z","shell.execute_reply.started":"2022-05-25T21:44:18.928380Z","shell.execute_reply":"2022-05-25T21:44:18.988585Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"Now do simple label encoding of our target variable Churn.","metadata":{}},{"cell_type":"code","source":"dfT['Churn'] = dfT['Churn'].map(lambda x: 1 if x == 'Yes' else 0)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:44:19.771354Z","iopub.execute_input":"2022-05-25T21:44:19.771958Z","iopub.status.idle":"2022-05-25T21:44:19.783596Z","shell.execute_reply.started":"2022-05-25T21:44:19.771886Z","shell.execute_reply":"2022-05-25T21:44:19.782467Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"## Balanced or imbalanced?\nLet's see if our dataset is balanced or imbalanced and if any action is needed. You will find out that data are highly imbalanced, we will use resample function to upsample minority group.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(5, 5))\nsns.countplot(dfT['Churn'])\nplt.title('Imbalanced dataset, it seems ratio is 2:5 for Yes:No')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:44:22.738042Z","iopub.execute_input":"2022-05-25T21:44:22.738666Z","iopub.status.idle":"2022-05-25T21:44:22.859945Z","shell.execute_reply.started":"2022-05-25T21:44:22.738628Z","shell.execute_reply":"2022-05-25T21:44:22.859057Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"Let's divide our data into 2 groups, majority (0) and minority (1) and create new dataset by upsampling minority group.","metadata":{}},{"cell_type":"code","source":"minority = dfT[dfT.Churn==1]\nmajority = dfT[dfT.Churn==0]\n\nminority_upsample = resample(minority, replace=True, n_samples=majority.shape[0])\ndfT = pd.concat([minority_upsample, majority], axis=0)\ndfT = dfT.sample(frac=1).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:44:27.619581Z","iopub.execute_input":"2022-05-25T21:44:27.619974Z","iopub.status.idle":"2022-05-25T21:44:27.639586Z","shell.execute_reply.started":"2022-05-25T21:44:27.619935Z","shell.execute_reply":"2022-05-25T21:44:27.638498Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"Let's just have a quick check how it looked like before balance and after balance.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nplt.subplot(1,2,1)\nsns.countplot(df['Churn'])\nplt.title('Imbalanced dataset')\n\nplt.subplot(1,2,2)\nsns.countplot(dfT['Churn'])\nplt.title('Balanced dataset')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:44:30.766168Z","iopub.execute_input":"2022-05-25T21:44:30.766580Z","iopub.status.idle":"2022-05-25T21:44:30.985447Z","shell.execute_reply.started":"2022-05-25T21:44:30.766543Z","shell.execute_reply":"2022-05-25T21:44:30.984323Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"## Time to Scale!\n\nMachine Learning algorithms are sensitive on data that are not normalized to same scale. We will use robust scaler which can nicely handle outliers, but standard scaler might work well too. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range).","metadata":{}},{"cell_type":"code","source":"rs = RobustScaler()\ndfT['tenure'] = rs.fit_transform(dfT['tenure'].values.reshape(-1,1))\ndfT['MonthlyCharges'] = rs.fit_transform(dfT['MonthlyCharges'].values.reshape(-1,1))\ndfT['TotalCharges'] = rs.fit_transform(dfT['TotalCharges'].values.reshape(-1,1))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:44:32.086772Z","iopub.execute_input":"2022-05-25T21:44:32.087226Z","iopub.status.idle":"2022-05-25T21:44:32.100204Z","shell.execute_reply.started":"2022-05-25T21:44:32.087185Z","shell.execute_reply":"2022-05-25T21:44:32.099023Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"markdown","source":"## Data Split\n\nSplit our data into train & test partitions. Train partition will be used to train ML model, test will be used to validate it's performance. *70% goes to train, 30% goes to test*. It could be also 80:20 or 60:40, but we choose 70:30 in our research.","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(dfT.drop('Churn', axis=1).values, dfT['Churn'].values, test_size=0.3)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:44:37.158307Z","iopub.execute_input":"2022-05-25T21:44:37.158700Z","iopub.status.idle":"2022-05-25T21:44:37.172362Z","shell.execute_reply.started":"2022-05-25T21:44:37.158667Z","shell.execute_reply":"2022-05-25T21:44:37.170845Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"# Modeling\n\nWe will use Logistic Classifier, Decision Tree, Random Forest, XGBoost, Neural Network\n","metadata":{}},{"cell_type":"markdown","source":"## Logistic Regression\n\nLogistic regression is a process of modeling the probability of a discrete outcome given an input variable. The most common logistic regression models a binary outcome; something that can take two values such as true/false, yes/no, and so on.  The technique can also be used in engineering, especially for predicting the probability of failure of a given process, system or product. ","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:44:40.635669Z","iopub.execute_input":"2022-05-25T21:44:40.636103Z","iopub.status.idle":"2022-05-25T21:44:40.726455Z","shell.execute_reply.started":"2022-05-25T21:44:40.636064Z","shell.execute_reply":"2022-05-25T21:44:40.725439Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"model_lg = LogisticRegression(max_iter=500,random_state=0, n_jobs=-1)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:44:41.411723Z","iopub.execute_input":"2022-05-25T21:44:41.412099Z","iopub.status.idle":"2022-05-25T21:44:41.416672Z","shell.execute_reply.started":"2022-05-25T21:44:41.412066Z","shell.execute_reply":"2022-05-25T21:44:41.415945Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"model_lg.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:44:41.875594Z","iopub.execute_input":"2022-05-25T21:44:41.875999Z","iopub.status.idle":"2022-05-25T21:44:42.944689Z","shell.execute_reply.started":"2022-05-25T21:44:41.875963Z","shell.execute_reply":"2022-05-25T21:44:42.943651Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"# Making Predictions\npred_lg = model_lg.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:44:42.947589Z","iopub.execute_input":"2022-05-25T21:44:42.947948Z","iopub.status.idle":"2022-05-25T21:44:42.954927Z","shell.execute_reply.started":"2022-05-25T21:44:42.947887Z","shell.execute_reply":"2022-05-25T21:44:42.954086Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, pred_lg))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:44:42.956447Z","iopub.execute_input":"2022-05-25T21:44:42.957063Z","iopub.status.idle":"2022-05-25T21:44:42.979689Z","shell.execute_reply.started":"2022-05-25T21:44:42.957010Z","shell.execute_reply":"2022-05-25T21:44:42.978522Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"## Decision Tree Classifier\n\nDecision Trees, the popular and time-tested method of applying logic to complex problems, where the variables are many and the options specific and dependent, have an important role to play within Machine Learning.\n\nWe will dedicate this paper to understanding why this reasonably humble technique has become such an important tool for data scientists. And we will start the debate by suggesting that Decision Trees are popular because they have two key properties, which are: \n* Simplicity: Decision Trees are simple, visually appealing and are easy to interpret.\n* Accuracy: Advance Decision Tree models show exceptional performance in predicting patterns in complex data.  ","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:44:46.513497Z","iopub.execute_input":"2022-05-25T21:44:46.514213Z","iopub.status.idle":"2022-05-25T21:44:46.613737Z","shell.execute_reply.started":"2022-05-25T21:44:46.514157Z","shell.execute_reply":"2022-05-25T21:44:46.612636Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"# Creating object of the model\nmodel_dt = DecisionTreeClassifier(max_depth=4, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:44:47.163619Z","iopub.execute_input":"2022-05-25T21:44:47.164067Z","iopub.status.idle":"2022-05-25T21:44:47.169414Z","shell.execute_reply.started":"2022-05-25T21:44:47.163963Z","shell.execute_reply":"2022-05-25T21:44:47.168169Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"model_dt.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:44:53.579358Z","iopub.execute_input":"2022-05-25T21:44:53.579739Z","iopub.status.idle":"2022-05-25T21:44:53.609052Z","shell.execute_reply.started":"2022-05-25T21:44:53.579704Z","shell.execute_reply":"2022-05-25T21:44:53.607939Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"pred_dt = model_dt.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:44:54.173593Z","iopub.execute_input":"2022-05-25T21:44:54.174362Z","iopub.status.idle":"2022-05-25T21:44:54.179468Z","shell.execute_reply.started":"2022-05-25T21:44:54.174317Z","shell.execute_reply":"2022-05-25T21:44:54.178609Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, pred_dt))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:44:54.647723Z","iopub.execute_input":"2022-05-25T21:44:54.648281Z","iopub.status.idle":"2022-05-25T21:44:54.664373Z","shell.execute_reply.started":"2022-05-25T21:44:54.648243Z","shell.execute_reply":"2022-05-25T21:44:54.663152Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest\n\nRandom forest consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction.  \n\nThe low correlation between models is the key. Uncorrelated models can produce ensemble predictions that are more accurate than any of the individual predictions. The reason for this wonderful effect is that the trees protect each other from their individual errors (as long as they don’t constantly all err in the same direction). While some trees may be wrong, many other trees will be right, so as a group the trees are able to move in the correct direction. So the prerequisites for random forest to perform well are:\n* There needs to be some actual signal in our features so that models built using those features do better than random guessing.\n* The predictions (and therefore the errors) made by the individual trees need to have low correlations with each other.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:45:00.671929Z","iopub.execute_input":"2022-05-25T21:45:00.672328Z","iopub.status.idle":"2022-05-25T21:45:00.705400Z","shell.execute_reply.started":"2022-05-25T21:45:00.672292Z","shell.execute_reply":"2022-05-25T21:45:00.704329Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"model_rf = RandomForestClassifier(n_estimators=400,min_samples_leaf=0.13, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:45:01.877708Z","iopub.execute_input":"2022-05-25T21:45:01.878293Z","iopub.status.idle":"2022-05-25T21:45:01.883872Z","shell.execute_reply.started":"2022-05-25T21:45:01.878240Z","shell.execute_reply":"2022-05-25T21:45:01.883142Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"model_rf.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:45:02.561913Z","iopub.execute_input":"2022-05-25T21:45:02.562427Z","iopub.status.idle":"2022-05-25T21:45:03.964077Z","shell.execute_reply.started":"2022-05-25T21:45:02.562393Z","shell.execute_reply":"2022-05-25T21:45:03.963217Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"# Making Prediction\npred_rf = model_rf.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:45:04.793383Z","iopub.execute_input":"2022-05-25T21:45:04.793777Z","iopub.status.idle":"2022-05-25T21:45:04.913076Z","shell.execute_reply.started":"2022-05-25T21:45:04.793740Z","shell.execute_reply":"2022-05-25T21:45:04.912117Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,pred_rf))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:45:06.237073Z","iopub.execute_input":"2022-05-25T21:45:06.237484Z","iopub.status.idle":"2022-05-25T21:45:06.255206Z","shell.execute_reply.started":"2022-05-25T21:45:06.237437Z","shell.execute_reply":"2022-05-25T21:45:06.253817Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost\n\nLet's start with popular XGB Classifier and check it's performance. The two main reasons to use XGBoost are flexibility, execution speed and model performance. XGBoost dominates structured or tabular datasets on classification and regression predictive modeling problems. Its strength doesn’t only come from the algorithm, but also from all the underlying system optimization.  ","metadata":{}},{"cell_type":"code","source":"xg = XGBClassifier()\nxg.fit(X_train, y_train)\ny_test_hat_xg = xg.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:45:11.983065Z","iopub.execute_input":"2022-05-25T21:45:11.983597Z","iopub.status.idle":"2022-05-25T21:45:12.730082Z","shell.execute_reply.started":"2022-05-25T21:45:11.983563Z","shell.execute_reply":"2022-05-25T21:45:12.729103Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_test_hat_xg))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T19:42:02.155226Z","iopub.execute_input":"2022-05-25T19:42:02.155589Z","iopub.status.idle":"2022-05-25T19:42:02.173443Z","shell.execute_reply.started":"2022-05-25T19:42:02.155556Z","shell.execute_reply":"2022-05-25T19:42:02.171983Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"markdown","source":"We could try some hyperparameter tunning with models above.","metadata":{}},{"cell_type":"markdown","source":"## Deep neural networks\nDeep Neural Networks (DNN) is a neural network with some level of complexity, usually at least has two layers, qualifies as a deep neural network, or deep net for short. Deep nets process data in complex ways by employing sophisticated math modeling.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau","metadata":{"execution":{"iopub.status.busy":"2022-05-25T19:42:06.514122Z","iopub.execute_input":"2022-05-25T19:42:06.514924Z","iopub.status.idle":"2022-05-25T19:42:06.520850Z","shell.execute_reply.started":"2022-05-25T19:42:06.514881Z","shell.execute_reply":"2022-05-25T19:42:06.519604Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"markdown","source":"We use sequential model with multiple dense & dropout layers.","metadata":{}},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.1))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.1))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.1))\n\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.45))\n\nmodel.add(Dense(1, activation='sigmoid'))\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, verbose=1,patience=10, min_lr=0.0000000001)\nearly_stopping_cb = EarlyStopping(patience=10, restore_best_weights=True)\n\nmodel.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(x=X_train, y=y_train, batch_size=128, epochs=100, validation_data=(X_test, y_test), callbacks=[early_stopping_cb, reduce_lr])\n","metadata":{"execution":{"iopub.status.busy":"2022-05-25T19:42:08.372247Z","iopub.execute_input":"2022-05-25T19:42:08.373029Z","iopub.status.idle":"2022-05-25T19:43:05.507087Z","shell.execute_reply.started":"2022-05-25T19:42:08.372983Z","shell.execute_reply":"2022-05-25T19:43:05.506079Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"markdown","source":"Model is trained, increasing dropout solves overfitting","metadata":{}},{"cell_type":"code","source":"y_test_hat_tf = model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T19:43:09.891552Z","iopub.execute_input":"2022-05-25T19:43:09.892107Z","iopub.status.idle":"2022-05-25T19:43:10.432246Z","shell.execute_reply.started":"2022-05-25T19:43:09.892072Z","shell.execute_reply":"2022-05-25T19:43:10.430903Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"markdown","source":"Output of prediction are probabilities, let's convert probabilities into 0/1","metadata":{}},{"cell_type":"code","source":"y_test_hat_tf2 = [1 if x > 0.5 else 0 for x in y_test_hat_tf ]","metadata":{"execution":{"iopub.status.busy":"2022-05-25T19:43:11.565906Z","iopub.execute_input":"2022-05-25T19:43:11.566321Z","iopub.status.idle":"2022-05-25T19:43:11.580401Z","shell.execute_reply.started":"2022-05-25T19:43:11.566286Z","shell.execute_reply":"2022-05-25T19:43:11.579435Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"markdown","source":"And finally checkout classification report!","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test, y_test_hat_tf2))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T19:43:19.167035Z","iopub.execute_input":"2022-05-25T19:43:19.167629Z","iopub.status.idle":"2022-05-25T19:43:19.187769Z","shell.execute_reply.started":"2022-05-25T19:43:19.167592Z","shell.execute_reply":"2022-05-25T19:43:19.186573Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"markdown","source":"Xgboost is slightly better than NN","metadata":{}},{"cell_type":"markdown","source":"# Summary  \n\nWe do some testing and applies 6 algorithms for the model and we get the best accuracy from XGBoost compared to the Logistic, Regression Tree, Random Forest and DNN.  \n\nThe result from the other testing also satisfied (more than 75%). It means that the result we got from the market analysis regarding churn prediction is satisfactory reliable and the result are:\n\n* Target more on young and middle-aged customers because they can adopt easily to modern tecnology and pretty much have budget to spend on the service\n* Offer extra discount for the returning customer or the one who decided to choose one or two years contract, it will make them likely to stay with the package and contract\n* Overall discout will make a big difference because price is the major factors for the customers.","metadata":{}},{"cell_type":"markdown","source":"# Recomendations and Limitations  \n\nRegarding the limitations, we should mention the following limitations from the model and also dataset.\nThe number of observations are enough, but if we could have more columns of features like the customers’ geographic and location, (maybe another important data), we can get more ideas and insight compared to what we have now.  \n\nThe dataset is a cross-sectional dataset, so this means that there are no time series factors inside it. The goal is to predict churn rate, thus we are good enough to have the option of contracts from monthly, one year to two years. This the best prediction we can provide to predict and make a decision for the market in the future. It has been proven by the result of algorithms that we use. ","metadata":{}},{"cell_type":"markdown","source":"# Resources\n\n1. Kaggle: https://www.kaggle.com/mytymohan/teleco-customer-churn-analysis\n2. https://www.kaggle.com/chinmaybgaikwad/customer-churn-in-telcos\n3. Dataset: https://www.kaggle.com/blastchar/telco-customer-churn\n4. Original dataset from IBM: https://www.ibm.com/docs/en/cognos-analytics/11.1.0?topic=samples-telco-customer-churn\n5. https://mode.com/blog/violin-plot-examples/\n6. https://towardsdatascience.com/understanding-random-forest-58381e0602d2\n7. https://towardsdatascience.com/xgboost-fine-tune-and-optimize-your-model-23d996fab663\n","metadata":{}}]}